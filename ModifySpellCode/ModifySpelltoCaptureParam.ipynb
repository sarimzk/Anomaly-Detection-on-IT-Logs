{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description : This file implements the Spell algorithm for log parsing\n",
    "Author      : LogPAI team\n",
    "License     : MIT\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class LCSObject:\n",
    "    \"\"\" Class object to store a log group with the same template\n",
    "    \"\"\"\n",
    "    def __init__(self, logTemplate='', logIDL=[]):\n",
    "        #print('J')\n",
    "        self.logTemplate = logTemplate\n",
    "        self.logIDL = logIDL\n",
    "        #print('logTemplate', logTemplate)\n",
    "        #print('logIDL',logIDL)\n",
    "        #print('back to A')\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\" A node in prefix tree data structure\n",
    "    \"\"\"\n",
    "    def __init__(self, token='', templateNo=0):\n",
    "        #print('E')\n",
    "        self.logClust = None\n",
    "        self.token = token\n",
    "        self.templateNo = templateNo\n",
    "        self.childD = dict()\n",
    "        self.makeList = dict()\n",
    "\n",
    "\n",
    "class LogParser:\n",
    "    \"\"\" LogParser class\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        path : the path of the input file\n",
    "        logName : the file name of the input file\n",
    "        savePath : the path of the output file\n",
    "        tau : how much percentage of tokens matched to merge a log message\n",
    "    \"\"\"\n",
    "    def __init__(self, indir='./', outdir='./result/', log_format=None, tau=0.5, rex=[], makeParamList=[]):\n",
    "        #print('3')\n",
    "        self.path = indir\n",
    "        self.logName = None\n",
    "        self.savePath = outdir\n",
    "        self.tau = tau\n",
    "        self.logformat = log_format\n",
    "        self.df_log = None\n",
    "        self.rex = rex\n",
    "        self.makeParamList= makeParamList\n",
    "    \n",
    "    def LCS(self, seq1, seq2):\n",
    "        #print('4')\n",
    "        lengths = [[0 for j in range(len(seq2)+1)] for i in range(len(seq1)+1)]\n",
    "        # row 0 and column 0 are initialized to 0 already\n",
    "        for i in range(len(seq1)):\n",
    "            for j in range(len(seq2)):\n",
    "                if seq1[i] == seq2[j]:\n",
    "                    lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "                else:\n",
    "                    lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "\n",
    "        # read the substring out from the matrix\n",
    "        result = []\n",
    "        lenOfSeq1, lenOfSeq2 = len(seq1), len(seq2)\n",
    "        while lenOfSeq1!=0 and lenOfSeq2 != 0:\n",
    "            if lengths[lenOfSeq1][lenOfSeq2] == lengths[lenOfSeq1-1][lenOfSeq2]:\n",
    "                lenOfSeq1 -= 1\n",
    "            elif lengths[lenOfSeq1][lenOfSeq2] == lengths[lenOfSeq1][lenOfSeq2-1]:\n",
    "                lenOfSeq2 -= 1\n",
    "            else:\n",
    "                assert seq1[lenOfSeq1-1] == seq2[lenOfSeq2-1]\n",
    "                result.insert(0,seq1[lenOfSeq1-1])\n",
    "                lenOfSeq1 -= 1\n",
    "                lenOfSeq2 -= 1\n",
    "        #print('LCS Result',result)\n",
    "\n",
    "        #print('back to A')\n",
    "        return result\n",
    "\n",
    "\n",
    "    def getTemplate(self, lcs, seq):\n",
    "        #print('8')\n",
    "        print('lcs',lcs, '\\n','seq',seq)\n",
    "        retVal = []\n",
    "        param = []\n",
    "        if not lcs:\n",
    "            return retVal\n",
    "\n",
    "        lcs = lcs[::-1]\n",
    "        i = 0\n",
    "        for token in seq:\n",
    "            i += 1\n",
    "            if token == lcs[-1]:\n",
    "                retVal.append(token)\n",
    "                #print('token',token, '--', 'retVal',retVal)\n",
    "                lcs.pop()\n",
    "            else:\n",
    "                param.append(token)\n",
    "                retVal.append('*')\n",
    "            if not lcs:\n",
    "                break\n",
    "        if i < len(seq):\n",
    "            retVal.append('*')\n",
    "        #print('back to A')\n",
    "        print('parameters:', param)\n",
    "        print('retVal',retVal)\n",
    "       \n",
    "        return retVal, param\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def removeSeqFromPrefixTree(self, rootn, newCluster):\n",
    "        #print('from 8')\n",
    "        #print('10')\n",
    "        parentn = rootn\n",
    "        seq = newCluster.logTemplate\n",
    "        seq = [w for w in seq if w != '*']\n",
    "\n",
    "        for tokenInSeq in seq:\n",
    "            if tokenInSeq in parentn.childD:\n",
    "                matchedNode = parentn.childD[tokenInSeq]\n",
    "                if matchedNode.templateNo == 1:\n",
    "                    del parentn.childD[tokenInSeq]\n",
    "                    break\n",
    "                else:\n",
    "                    matchedNode.templateNo -= 1\n",
    "                    parentn = matchedNode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def printTree(self, node, dep):\n",
    "        #print('12')\n",
    "        pStr = ''\n",
    "        for i in range(dep):\n",
    "            pStr += '\\t'\n",
    "\n",
    "        if node.token == '':\n",
    "            pStr += 'Root'\n",
    "        else:\n",
    "            pStr += node.token\n",
    "            if node.logClust is not None:\n",
    "                pStr += '-->' + ' '.join(node.logClust.logTemplate)\n",
    "        #print((pStr +' ('+ str(node.templateNo) + ')'))\n",
    "\n",
    "        for child in node.childD:\n",
    "            self.printTree(node.childD[child], dep + 1)\n",
    "\n",
    "\n",
    "    def parse(self, logname):\n",
    "        #print('A')\n",
    "        starttime = datetime.now()\n",
    "        #print(('Parsing file: ' + os.path.join(self.path, logname)))\n",
    "        self.logname = logname\n",
    "        self.load_data()\n",
    "        #print('to B')\n",
    "        rootNode = Node()\n",
    "        logCluL = []\n",
    "\n",
    "        count = 0\n",
    "        for idx, line in self.df_log.iterrows():\n",
    "            logID = line['LineId']\n",
    "            #print('logid',logID)\n",
    "            #print('to F')             \n",
    "            logmessageL = [x for x in re.split(r'[\\s=:,]', self.preprocess(line['Content'])) if x != '']\n",
    "            #print('logmessageL',logmessageL)\n",
    "            constLogMessL = [w for w in logmessageL if w != '*']\n",
    "            #print('constLogMessL',constLogMessL)\n",
    "\n",
    "            #Find an existing matched log cluster\n",
    "            matchCluster = self.PrefixTreeMatch(rootNode, constLogMessL, 0)\n",
    "            #print('to G')\n",
    "\n",
    "            if matchCluster is None:\n",
    "                #print('to H')\n",
    "                matchCluster = self.SimpleLoopMatch(logCluL, constLogMessL)\n",
    "                #print('matchCluster1',matchCluster)\n",
    "\n",
    "                if matchCluster is None:\n",
    "                    #print('to I')\n",
    "                    matchCluster = self.LCSMatch(logCluL, logmessageL)\n",
    "                    #print('matchCluster2',matchCluster)\n",
    "\n",
    "                    # Match no existing log cluster\n",
    "                    if matchCluster is None:\n",
    "                        #print('to J')\n",
    "                        newCluster = LCSObject(logTemplate=logmessageL, logIDL=[logID])\n",
    "                        logCluL.append(newCluster)\n",
    "                        self.addSeqToPrefixTree(rootNode, newCluster)\n",
    "                    #Add the new log message to the existing cluster\n",
    "                    else:\n",
    "                        #print('to 4 and then 8')\n",
    "                        newTemplate, param = self.getTemplate(self.LCS(logmessageL, matchCluster.logTemplate),\n",
    "                                                       matchCluster.logTemplate)\n",
    "                        makeParamList.append(param)\n",
    "                        if ' '.join(newTemplate) != ' '.join(matchCluster.logTemplate):\n",
    "                            self.removeSeqFromPrefixTree(rootNode, matchCluster)\n",
    "                            matchCluster.logTemplate = newTemplate\n",
    "                            self.addSeqToPrefixTree(rootNode, matchCluster)\n",
    "            if matchCluster:\n",
    "                matchCluster.logIDL.append(logID)\n",
    "                #print('matchCluster.logIDL.append(logID)',matchCluster.logIDL.append(logID))\n",
    "            count += 1\n",
    "            if count % 1000 == 0 or count == len(self.df_log):\n",
    "                print(('Processed {0:.1f}% of log lines.'.format(count * 100.0 / len(self.df_log))))\n",
    "                #print('testing')\n",
    "                #print('makeParamList___:',makeParamList)\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "\n",
    "        self.outputResult(logCluL,makeParamList)\n",
    "        print(('Parsing done. [Time taken: {!s}]'.format(datetime.now() - starttime)))\n",
    "\n",
    "    def load_data(self):\n",
    "        #print('B')\n",
    "        #print('to C')\n",
    "        headers, regex = self.generate_logformat_regex(self.logformat)\n",
    "        \n",
    "        #print('headers', headers,'regex',  regex\n",
    "        \n",
    "        self.df_log = self.log_to_dataframe(os.path.join(self.path, self.logname), regex, headers, self.logformat)\n",
    "        #print('self.df_log', self.df_log)\n",
    "        #print('Back to A')\n",
    "\n",
    "    def generate_logformat_regex(self, logformat):\n",
    "        #print('C')\n",
    "        \"\"\" Function to generate regular expression to split log messages\n",
    "        \"\"\"\n",
    "        headers = []\n",
    "        splitters = re.split(r'(<[^<>]+>)', logformat)\n",
    "        regex = ''\n",
    "        for k in range(len(splitters)):\n",
    "            if k % 2 == 0:\n",
    "                splitter = re.sub(' +', '\\s+', splitters[k])\n",
    "                regex += splitter\n",
    "            else:\n",
    "                header = splitters[k].strip('<').strip('>')\n",
    "                regex += '(?P<%s>.*?)' % header\n",
    "                headers.append(header)\n",
    "        regex = re.compile('^' + regex + '$')\n",
    "        return headers, regex\n",
    "\n",
    "    def log_to_dataframe(self, log_file, regex, headers, logformat):\n",
    "        #print('D')\n",
    "        \"\"\" Function to transform log file to dataframe\n",
    "        \"\"\"\n",
    "        log_messages = []\n",
    "        linecount = 0\n",
    "        with open(log_file, 'r') as fin:\n",
    "            for line in fin.readlines():\n",
    "                line = re.sub(r'[^\\x00-\\x7F]+', '<NASCII>', line)\n",
    "                try:\n",
    "                    match = regex.search(line.strip())\n",
    "                    message = [match.group(header) for header in headers]\n",
    "                    log_messages.append(message)\n",
    "                    linecount += 1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "        logdf.insert(0, 'LineId', None)\n",
    "        logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "        #print('back to B')\n",
    "        #print('logdf',logdf)\n",
    "        return logdf\n",
    "\n",
    "    def preprocess(self, line):\n",
    "        #print('F')\n",
    "        for currentRex in self.rex:\n",
    "            #print('currentRex', currentRex)\n",
    "            line = re.sub(currentRex, '*', line)\n",
    "            #print('line', line)\n",
    "        #print('Back to A')\n",
    "        return line\n",
    "\n",
    "\n",
    "    def PrefixTreeMatch(self, parentn, seq, idx):\n",
    "        #print('G')\n",
    "        retLogClust = None\n",
    "        length = len(seq)\n",
    "        for i in range(idx, length):\n",
    "            if seq[i] in parentn.childD:\n",
    "                childn = parentn.childD[seq[i]]\n",
    "                #print(\"Childn:\", childn)\n",
    "                if (childn.logClust is not None):\n",
    "                    constLM = [w for w in childn.logClust.logTemplate if w != '*']\n",
    "                    #print(\"ConstLM:\",constLM)\n",
    "                    if float(len(constLM)) >= self.tau * length:\n",
    "                        #print('childn.logClust', childn.logClust)\n",
    "                        return childn.logClust\n",
    "                else:\n",
    "                    #print(self.PrefixTreeMatch(childn, seq, i + 1))\n",
    "                    return self.PrefixTreeMatch(childn, seq, i + 1)\n",
    "        #print('Back to A')\n",
    "        #print('retLogClust',retLogClust)\n",
    "        return retLogClust\n",
    "\n",
    "\n",
    "    def SimpleLoopMatch(self, logClustL, seq):\n",
    "        #print('H')\n",
    "        retLogClust = None\n",
    "\n",
    "        for logClust in logClustL:\n",
    "            if float(len(logClust.logTemplate)) < 0.5 * len(seq):\n",
    "                continue\n",
    "\n",
    "            #If the template is a subsequence of seq\n",
    "            it = iter(seq)\n",
    "            if all(token in seq or token == '*' for token in logClust.logTemplate):\n",
    "                return logClust\n",
    "        #print('Back to A')\n",
    "        return retLogClust\n",
    "\n",
    "\n",
    "    def LCSMatch(self, logClustL, seq):\n",
    "        #print('I')\n",
    "        retLogClust = None\n",
    "\n",
    "        maxLen = -1\n",
    "        maxlcs = []\n",
    "        maxClust = None\n",
    "        set_seq = set(seq)\n",
    "        size_seq = len(seq)\n",
    "        for logClust in logClustL:\n",
    "            set_template = set(logClust.logTemplate)\n",
    "            if len(set_seq & set_template) < 0.5 * size_seq:\n",
    "                continue\n",
    "            lcs = self.LCS(seq, logClust.logTemplate)\n",
    "            if len(lcs) > maxLen or (len(lcs) == maxLen and len(logClust.logTemplate) < len(maxClust.logTemplate)):\n",
    "                maxLen = len(lcs)\n",
    "                maxlcs = lcs\n",
    "                maxClust = logClust\n",
    "\n",
    "        # LCS should be large then tau * len(itself)\n",
    "        if float(maxLen) >= self.tau * size_seq:\n",
    "            retLogClust = maxClust\n",
    "        #print('Back to A')\n",
    "        return retLogClust\n",
    "\n",
    "    def addSeqToPrefixTree(self, rootn, newCluster):\n",
    "        #print('K')\n",
    "        parentn = rootn\n",
    "        seq = newCluster.logTemplate\n",
    "        seq = [w for w in seq if w != '*']\n",
    "\n",
    "        for i in range(len(seq)):\n",
    "            tokenInSeq = seq[i]\n",
    "            # Match\n",
    "            if tokenInSeq in parentn.childD:\n",
    "                parentn.childD[tokenInSeq].templateNo += 1\n",
    "            # Do not Match\n",
    "            else:\n",
    "                parentn.childD[tokenInSeq] = Node(token=tokenInSeq, templateNo=1)\n",
    "            parentn = parentn.childD[tokenInSeq]\n",
    "\n",
    "        if parentn.logClust is None:\n",
    "            parentn.logClust = newCluster\n",
    "\n",
    "    \n",
    "    def outputResult(self, logClustL, makeParamList):\n",
    "        #print('Z')\n",
    "        #print('Parameters:',makeParamList)\n",
    "        templates = [0] * self.df_log.shape[0]\n",
    "        ids = [0] * self.df_log.shape[0]\n",
    "        df_event = []\n",
    "\n",
    "        for logclust in logClustL:\n",
    "            template_str = ' '.join(logclust.logTemplate)\n",
    "            eid = hashlib.md5(template_str.encode('utf-8')).hexdigest()[0:8]\n",
    "            for logid in logclust.logIDL:\n",
    "                templates[logid - 1] = template_str\n",
    "                ids[logid - 1] = eid\n",
    "            df_event.append([eid, template_str, len(logclust.logIDL)])\n",
    "\n",
    "        df_event = pd.DataFrame(df_event, columns=['EventId', 'EventTemplate', 'Occurrences'])\n",
    "\n",
    "        self.df_log['EventId'] = ids\n",
    "        self.df_log['EventTemplate'] = templates\n",
    "        self.df_log.to_csv(os.path.join(self.savePath, self.logname + '_structured.csv'), index=False)\n",
    "        df_event.to_csv(os.path.join(self.savePath, self.logname + '_templates.csv'), index=False)\n",
    "        #dfP= pd.DataFrame(makeParamList)\n",
    "        #dfP.to_csv(os.path.join(self.savePath, self.makeParamList ), index=False)\n",
    "        #dfP.to_pickle('makeParamList.pkl') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir  = '/Users/vikrant/jupyternotebooks/'  # The input directory of log file\n",
    "output_dir = 'Spell_result/11/'  # The output directory of parsing results\n",
    "log_file   = '1k.csv'  # The input log file name\n",
    "log_format = '<Time> <Machine> <Daemon> <Content>'  # HDFS log format\n",
    "tau        = 0.6  # Message type threshold (default: 0.5)\n",
    "regex      = []  # Regular expression list for optional preprocessing (default: [])\n",
    "makeParamList =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Spell\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "parser =LogParser( makeParamList=makeParamList,indir=input_dir,outdir=output_dir, log_format=log_format, tau=tau, rex=regex,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcs ['a4', 'ba', 'db', 'via', 'network', 'no', 'free', 'leases\"'] \n",
      " seq ['a4', 'ba', 'db', '84', '1d', '63', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "parameters: ['84', '1d', '63', 'eth0', '206.76.192/20']\n",
      "retVal ['a4', 'ba', 'db', '*', '*', '*', 'via', '*', 'network', '*', 'no', 'free', 'leases\"']\n",
      "lcs ['repeated', 'times\"'] \n",
      " seq ['repeated', '2', 'times\"']\n",
      "parameters: ['2']\n",
      "retVal ['repeated', '*', 'times\"']\n",
      "lcs ['206.76.199.180', '00', '26', 'b9', 'fe', '52', '30', 'via', 'eth0\"'] \n",
      " seq ['206.76.199.180', 'from', '00', '26', 'b9', 'fe', '52', '30', 'via', 'eth0\"']\n",
      "parameters: ['from']\n",
      "retVal ['206.76.199.180', '*', '00', '26', 'b9', 'fe', '52', '30', 'via', 'eth0\"']\n",
      "lcs ['eth0', 'to', '10.4.0.1', 'port', '67', '(xid'] \n",
      " seq ['eth0', 'to', '10.4.0.1', 'port', '67', '(xid', '0x6e318336)\"']\n",
      "parameters: []\n",
      "retVal ['eth0', 'to', '10.4.0.1', 'port', '67', '(xid', '*']\n",
      "lcs ['10.4.192.252', 'a4', 'ba', 'db', '01', '67', 'a2', 'via', 'eth2\"'] \n",
      " seq ['10.4.192.252', 'from', 'a4', 'ba', 'db', '01', '67', 'a2', 'via', 'eth2\"']\n",
      "parameters: ['from']\n",
      "retVal ['10.4.192.252', '*', 'a4', 'ba', 'db', '01', '67', 'a2', 'via', 'eth2\"']\n",
      "lcs ['a4', 'ba', 'db', '01', 'via', 'eth2\"'] \n",
      " seq ['10.4.192.252', '*', 'a4', 'ba', 'db', '01', '67', 'a2', 'via', 'eth2\"']\n",
      "parameters: ['10.4.192.252', '*', '67', 'a2']\n",
      "retVal ['*', '*', 'a4', 'ba', 'db', '01', '*', '*', 'via', 'eth2\"']\n",
      "lcs ['10.4.194.182', 'a4', 'ba', 'db', '01', '5c', '6e', 'via', 'eth0\"'] \n",
      " seq ['10.4.194.182', 'from', 'a4', 'ba', 'db', '01', '5c', '6e', 'via', 'eth0\"']\n",
      "parameters: ['from']\n",
      "retVal ['10.4.194.182', '*', 'a4', 'ba', 'db', '01', '5c', '6e', 'via', 'eth0\"']\n",
      "lcs ['00', '8c', 'fa', '0e', '9f', 'a7', 'via', 'network', 'no', 'free', 'leases\"'] \n",
      " seq ['00', '8c', 'fa', '0e', '9f', 'a7', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "parameters: ['eth0', '206.76.192/20']\n",
      "retVal ['00', '8c', 'fa', '0e', '9f', 'a7', 'via', '*', 'network', '*', 'no', 'free', 'leases\"']\n",
      "lcs ['84', '2b', '2b', '4e', 'cf', '44', 'via', 'network', 'no', 'free', 'leases\"'] \n",
      " seq ['84', '2b', '2b', '4e', 'cf', '44', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "parameters: ['eth0', '206.76.192/20']\n",
      "retVal ['84', '2b', '2b', '4e', 'cf', '44', 'via', '*', 'network', '*', 'no', 'free', 'leases\"']\n",
      "lcs ['a4', 'ba', 'db', '01', 'via', 'eth0\"'] \n",
      " seq ['10.4.194.182', '*', 'a4', 'ba', 'db', '01', '5c', '6e', 'via', 'eth0\"']\n",
      "parameters: ['10.4.194.182', '*', '5c', '6e']\n",
      "retVal ['*', '*', 'a4', 'ba', 'db', '01', '*', '*', 'via', 'eth0\"']\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.226223]\n"
     ]
    }
   ],
   "source": [
    "parser.parse(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: /Users/vikrant/jupyternotebooks/10.csv\n",
      "headers ['Time', 'Machine', 'Daemon', 'Content'] regex re.compile('^(?P<Time>.*?)\\\\s+(?P<Machine>.*?)\\\\s+(?P<Daemon>.*?)\\\\s+(?P<Content>.*?)$')\n",
      "self.df_log    LineId             Time                                   Machine Daemon  \\\n",
      "0       1  \"0\",\"2013-03-10  00:00:00\",\"master\",\"dhcpd\",\"DHCPDISCOVER   from   \n",
      "1       2  \"1\",\"2013-03-10  00:00:00\",\"master\",\"dhcpd\",\"DHCPDISCOVER   from   \n",
      "2       3  \"2\",\"2013-03-10  00:00:00\",\"master\",\"dhcpd\",\"DHCPDISCOVER   from   \n",
      "3       4  \"3\",\"2013-03-10  00:00:00\",\"master\",\"dhcpd\",\"DHCPDISCOVER   from   \n",
      "4       5  \"4\",\"2013-03-10  00:00:00\",\"master\",\"dhcpd\",\"DHCPDISCOVER   from   \n",
      "5       6  \"5\",\"2013-03-10      00:00:00\",\"oss10\",\"multipathd\",\"sdq:    tur   \n",
      "6       7  \"6\",\"2013-03-10      00:00:00\",\"oss11\",\"multipathd\",\"sdu:    tur   \n",
      "7       8  \"7\",\"2013-03-10     00:00:00\",\"oss11\",\"multipathd\",\"sdam:    tur   \n",
      "8       9  \"8\",\"2013-03-10     00:00:00\",\"oss11\",\"multipathd\",\"sdax:    tur   \n",
      "\n",
      "                                             Content  \n",
      "0  a4:ba:db:84:1d:63 via eth0: network 206.76.192...  \n",
      "1  a4:ba:db:80:24:54 via eth2: network 10.4/16: n...  \n",
      "2  a4:ba:db:80:24:54 via eth0: network 206.76.192...  \n",
      "3  a4:ba:db:74:3e:1e via eth0: network 206.76.192...  \n",
      "4  a4:ba:db:74:3e:1e via eth2: network 10.4/16: n...  \n",
      "5                      checker reports path is down\"  \n",
      "6                      checker reports path is down\"  \n",
      "7                      checker reports path is down\"  \n",
      "8                      checker reports path is down\"  \n",
      "['a4', 'ba', 'db', '84', '1d', '63', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '84', '1d', '63', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '80', '24', '54', 'via', 'eth2', 'network', '10.4/16', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '80', '24', '54', 'via', 'eth2', 'network', '10.4/16', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '80', '24', '54', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '80', '24', '54', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '74', '3e', '1e', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '74', '3e', '1e', 'via', 'eth0', 'network', '206.76.192/20', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '74', '3e', '1e', 'via', 'eth2', 'network', '10.4/16', 'no', 'free', 'leases\"']\n",
      "['a4', 'ba', 'db', '74', '3e', '1e', 'via', 'eth2', 'network', '10.4/16', 'no', 'free', 'leases\"']\n",
      "['checker', 'reports', 'path', 'is', 'down\"']\n",
      "['checker', 'reports', 'path', 'is', 'down\"']\n",
      "['checker', 'reports', 'path', 'is', 'down\"']\n",
      "['checker', 'reports', 'path', 'is', 'down\"']\n",
      "['checker', 'reports', 'path', 'is', 'down\"']\n",
      "['checker', 'reports', 'path', 'is', 'down\"']\n",
      "['checker', 'reports', 'path', 'is', 'down\"']\n",
      "['checker', 'reports', 'path', 'is', 'down\"']\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.021638]\n"
     ]
    }
   ],
   "source": [
    "parser.parse(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
